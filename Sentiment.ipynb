{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train=pd.read_csv(\"train.tsv\",sep=\"\\t\")\n",
    "test=pd.read_csv(\"test.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n",
    "# statistical natural language processing for English written in the Python programming language.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#TQDM is a progress bar library with good support for nested loops and Jupyter/IPython notebooks.\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import random\n",
    "from tensorflow import set_random_seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "#set random seed for the session and also for tensorflow that runs in background for keras\n",
    "set_random_seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66292, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(df):\n",
    "    reviews = []\n",
    "\n",
    "    for sent in tqdm(df['Phrase']):\n",
    "        \n",
    "        #remove html content\n",
    "        review_text = BeautifulSoup(sent).get_text()\n",
    "        \n",
    "        #remove non-alphabetic characters\n",
    "        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    \n",
    "        #tokenize the sentences\n",
    "        words = word_tokenize(review_text.lower())\n",
    "    \n",
    "        #lemmatize each word to its lemma\n",
    "        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n",
    "    \n",
    "        reviews.append(lemma_words)\n",
    "\n",
    "    return(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 1/156060 [00:01<56:40:48,  1.31s/it]D:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "  0%|▎                                                                         | 628/156060 [00:01<27:40:07,  1.56it/s]D:\\Anaconda\\lib\\site-packages\\bs4\\__init__.py:272: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "100%|████████████████████████████████████████████████████████████████████████| 156060/156060 [00:55<00:00, 2811.60it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 66292/66292 [00:23<00:00, 2877.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060\n",
      "66292\n"
     ]
    }
   ],
   "source": [
    "#cleaned reviews for both train and test set retrieved\n",
    "train_sentences = clean_sentences(train)\n",
    "test_sentences = clean_sentences(test)\n",
    "print(len(train_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=train.Sentiment.values\n",
    "y_target=to_categorical(target)\n",
    "num_classes=y_target.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_val,y_train,y_val=train_test_split(train_sentences,y_target,test_size=0.2,stratify=y_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 124848/124848 [00:00<00:00, 767984.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13737\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "#It is needed for initializing tokenizer of keras and subsequent padding\n",
    "\n",
    "unique_words = set()\n",
    "len_max = 0\n",
    "\n",
    "for sent in tqdm(X_train):\n",
    "    \n",
    "    unique_words.update(sent)\n",
    "    \n",
    "    if(len_max<len(sent)):\n",
    "        len_max = len(sent)\n",
    "        \n",
    "#length of the list of unique_words gives the no of unique words\n",
    "print(len(list(unique_words)))\n",
    "print(len_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 48) (31212, 48) (66292, 48)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=len(list(unique_words)))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "\n",
    "#texts_to_sequences(texts)\n",
    "\n",
    "    # Arguments- texts: list of texts to turn to sequences.\n",
    "    #Return: list of sequences (one per text input).\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "#padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n",
    "#Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=len_max)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=len_max)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=len_max)\n",
    "\n",
    "print(X_train.shape,X_val.shape,X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\n",
    "callback = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0207 21:15:39.656015  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0207 21:15:39.660007  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0207 21:15:39.663025  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0207 21:15:40.060930  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0207 21:15:40.068908  3156 deprecation.py:506] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0207 21:15:40.669303  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0207 21:15:40.676285  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 48, 300)           4121100   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 48, 128)           219648    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 4,397,161\n",
      "Trainable params: 4,397,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model using Keras LSTM\n",
    "\n",
    "#Multilayer Perceptron (MLP) for multi-class softmax classification:\n",
    "#Let’s build what’s probably the most popular type of model in NLP at the moment: Long Short Term Memory network. \n",
    "#This architecture is specially designed to work on sequence data.\n",
    "#It fits perfectly for many NLP tasks like tagging and text classification.\n",
    "#It treats the text as a sequence rather than a bag of words or as ngrams.\n",
    "\n",
    "#Here’s a possible model definition:\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(len(list(unique_words)),300,input_length=len_max))\n",
    "model.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0207 21:17:57.705650  3156 deprecation.py:323] From D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0207 21:17:59.030078  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0207 21:17:59.170703  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "W0207 21:17:59.288420  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0207 21:17:59.295369  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0207 21:17:59.297367  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0207 21:17:59.478879  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0207 21:17:59.479877  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0207 21:17:59.636457  3156 module_wrapper.py:139] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 52224/124848 [===========>..................] - ETA: 16:12 - loss: 1.6095 - acc: 0.22 - ETA: 9:49 - loss: 1.5632 - acc: 0.3516 - ETA: 7:43 - loss: 1.5033 - acc: 0.399 - ETA: 6:40 - loss: 1.4551 - acc: 0.426 - ETA: 6:06 - loss: 1.4175 - acc: 0.442 - ETA: 5:37 - loss: 1.3853 - acc: 0.449 - ETA: 5:14 - loss: 1.3735 - acc: 0.457 - ETA: 4:58 - loss: 1.3788 - acc: 0.450 - ETA: 4:45 - loss: 1.3724 - acc: 0.451 - ETA: 4:34 - loss: 1.3589 - acc: 0.457 - ETA: 4:26 - loss: 1.3477 - acc: 0.463 - ETA: 4:19 - loss: 1.3412 - acc: 0.469 - ETA: 4:13 - loss: 1.3328 - acc: 0.475 - ETA: 4:08 - loss: 1.3258 - acc: 0.479 - ETA: 4:03 - loss: 1.3222 - acc: 0.483 - ETA: 3:59 - loss: 1.3262 - acc: 0.483 - ETA: 3:56 - loss: 1.3211 - acc: 0.486 - ETA: 3:53 - loss: 1.3203 - acc: 0.488 - ETA: 3:50 - loss: 1.3222 - acc: 0.486 - ETA: 3:47 - loss: 1.3188 - acc: 0.487 - ETA: 3:44 - loss: 1.3180 - acc: 0.487 - ETA: 3:42 - loss: 1.3151 - acc: 0.486 - ETA: 3:40 - loss: 1.3087 - acc: 0.488 - ETA: 3:39 - loss: 1.3056 - acc: 0.488 - ETA: 3:36 - loss: 1.3015 - acc: 0.491 - ETA: 3:35 - loss: 1.2955 - acc: 0.494 - ETA: 3:34 - loss: 1.2934 - acc: 0.494 - ETA: 3:33 - loss: 1.2910 - acc: 0.495 - ETA: 3:31 - loss: 1.2908 - acc: 0.495 - ETA: 3:30 - loss: 1.2904 - acc: 0.495 - ETA: 3:29 - loss: 1.2891 - acc: 0.495 - ETA: 3:28 - loss: 1.2873 - acc: 0.496 - ETA: 3:27 - loss: 1.2850 - acc: 0.495 - ETA: 3:25 - loss: 1.2833 - acc: 0.495 - ETA: 3:25 - loss: 1.2809 - acc: 0.496 - ETA: 3:24 - loss: 1.2773 - acc: 0.497 - ETA: 3:23 - loss: 1.2750 - acc: 0.499 - ETA: 3:22 - loss: 1.2722 - acc: 0.501 - ETA: 3:21 - loss: 1.2714 - acc: 0.502 - ETA: 3:20 - loss: 1.2698 - acc: 0.503 - ETA: 3:19 - loss: 1.2677 - acc: 0.504 - ETA: 3:19 - loss: 1.2662 - acc: 0.505 - ETA: 3:18 - loss: 1.2642 - acc: 0.506 - ETA: 3:17 - loss: 1.2639 - acc: 0.506 - ETA: 3:16 - loss: 1.2633 - acc: 0.507 - ETA: 3:16 - loss: 1.2605 - acc: 0.509 - ETA: 3:15 - loss: 1.2581 - acc: 0.510 - ETA: 3:14 - loss: 1.2585 - acc: 0.509 - ETA: 3:14 - loss: 1.2575 - acc: 0.509 - ETA: 3:13 - loss: 1.2558 - acc: 0.510 - ETA: 3:13 - loss: 1.2556 - acc: 0.510 - ETA: 3:13 - loss: 1.2527 - acc: 0.511 - ETA: 3:14 - loss: 1.2526 - acc: 0.511 - ETA: 3:13 - loss: 1.2525 - acc: 0.511 - ETA: 3:13 - loss: 1.2513 - acc: 0.511 - ETA: 3:12 - loss: 1.2489 - acc: 0.512 - ETA: 3:12 - loss: 1.2487 - acc: 0.511 - ETA: 3:11 - loss: 1.2467 - acc: 0.512 - ETA: 3:10 - loss: 1.2466 - acc: 0.512 - ETA: 3:10 - loss: 1.2452 - acc: 0.513 - ETA: 3:09 - loss: 1.2455 - acc: 0.512 - ETA: 3:08 - loss: 1.2452 - acc: 0.513 - ETA: 3:08 - loss: 1.2445 - acc: 0.513 - ETA: 3:07 - loss: 1.2431 - acc: 0.513 - ETA: 3:07 - loss: 1.2404 - acc: 0.514 - ETA: 3:06 - loss: 1.2398 - acc: 0.514 - ETA: 3:06 - loss: 1.2390 - acc: 0.514 - ETA: 3:05 - loss: 1.2378 - acc: 0.514 - ETA: 3:04 - loss: 1.2372 - acc: 0.514 - ETA: 3:04 - loss: 1.2363 - acc: 0.514 - ETA: 3:03 - loss: 1.2354 - acc: 0.514 - ETA: 3:03 - loss: 1.2347 - acc: 0.514 - ETA: 3:02 - loss: 1.2342 - acc: 0.514 - ETA: 3:02 - loss: 1.2339 - acc: 0.514 - ETA: 3:01 - loss: 1.2336 - acc: 0.514 - ETA: 3:01 - loss: 1.2341 - acc: 0.514 - ETA: 3:00 - loss: 1.2328 - acc: 0.514 - ETA: 3:00 - loss: 1.2325 - acc: 0.514 - ETA: 2:59 - loss: 1.2310 - acc: 0.515 - ETA: 2:59 - loss: 1.2297 - acc: 0.515 - ETA: 2:58 - loss: 1.2272 - acc: 0.516 - ETA: 2:58 - loss: 1.2266 - acc: 0.517 - ETA: 2:57 - loss: 1.2261 - acc: 0.517 - ETA: 2:57 - loss: 1.2256 - acc: 0.517 - ETA: 2:56 - loss: 1.2250 - acc: 0.518 - ETA: 2:56 - loss: 1.2240 - acc: 0.517 - ETA: 2:55 - loss: 1.2230 - acc: 0.518 - ETA: 2:55 - loss: 1.2221 - acc: 0.518 - ETA: 2:55 - loss: 1.2210 - acc: 0.519 - ETA: 2:54 - loss: 1.2202 - acc: 0.519 - ETA: 2:54 - loss: 1.2192 - acc: 0.519 - ETA: 2:53 - loss: 1.2187 - acc: 0.520 - ETA: 2:53 - loss: 1.2171 - acc: 0.521 - ETA: 2:52 - loss: 1.2176 - acc: 0.520 - ETA: 2:52 - loss: 1.2166 - acc: 0.521 - ETA: 2:51 - loss: 1.2160 - acc: 0.521 - ETA: 2:51 - loss: 1.2153 - acc: 0.521 - ETA: 2:50 - loss: 1.2138 - acc: 0.522 - ETA: 2:50 - loss: 1.2121 - acc: 0.523 - ETA: 2:50 - loss: 1.2112 - acc: 0.523 - ETA: 2:49 - loss: 1.2091 - acc: 0.524 - ETA: 2:49 - loss: 1.2079 - acc: 0.525 - ETA: 2:48 - loss: 1.2067 - acc: 0.525 - ETA: 2:48 - loss: 1.2054 - acc: 0.526 - ETA: 2:47 - loss: 1.2044 - acc: 0.526 - ETA: 2:47 - loss: 1.2034 - acc: 0.527 - ETA: 2:46 - loss: 1.2033 - acc: 0.527 - ETA: 2:46 - loss: 1.2016 - acc: 0.528 - ETA: 2:45 - loss: 1.2006 - acc: 0.529 - ETA: 2:45 - loss: 1.1994 - acc: 0.529 - ETA: 2:45 - loss: 1.1985 - acc: 0.529 - ETA: 2:44 - loss: 1.1987 - acc: 0.529 - ETA: 2:44 - loss: 1.1980 - acc: 0.529 - ETA: 2:43 - loss: 1.1966 - acc: 0.529 - ETA: 2:43 - loss: 1.1960 - acc: 0.529 - ETA: 2:42 - loss: 1.1950 - acc: 0.529 - ETA: 2:42 - loss: 1.1935 - acc: 0.530 - ETA: 2:41 - loss: 1.1924 - acc: 0.531 - ETA: 2:41 - loss: 1.1909 - acc: 0.531 - ETA: 2:41 - loss: 1.1898 - acc: 0.531 - ETA: 2:40 - loss: 1.1882 - acc: 0.532 - ETA: 2:40 - loss: 1.1870 - acc: 0.533 - ETA: 2:39 - loss: 1.1860 - acc: 0.533 - ETA: 2:39 - loss: 1.1851 - acc: 0.534 - ETA: 2:38 - loss: 1.1841 - acc: 0.534 - ETA: 2:38 - loss: 1.1836 - acc: 0.534 - ETA: 2:37 - loss: 1.1829 - acc: 0.534 - ETA: 2:37 - loss: 1.1820 - acc: 0.535 - ETA: 2:36 - loss: 1.1807 - acc: 0.535 - ETA: 2:36 - loss: 1.1791 - acc: 0.536 - ETA: 2:36 - loss: 1.1779 - acc: 0.537 - ETA: 2:35 - loss: 1.1763 - acc: 0.537 - ETA: 2:35 - loss: 1.1757 - acc: 0.538 - ETA: 2:34 - loss: 1.1743 - acc: 0.538 - ETA: 2:34 - loss: 1.1732 - acc: 0.539 - ETA: 2:33 - loss: 1.1724 - acc: 0.539 - ETA: 2:33 - loss: 1.1715 - acc: 0.540 - ETA: 2:32 - loss: 1.1711 - acc: 0.540 - ETA: 2:32 - loss: 1.1704 - acc: 0.540 - ETA: 2:32 - loss: 1.1692 - acc: 0.540 - ETA: 2:31 - loss: 1.1683 - acc: 0.541 - ETA: 2:31 - loss: 1.1677 - acc: 0.541 - ETA: 2:30 - loss: 1.1671 - acc: 0.541 - ETA: 2:30 - loss: 1.1663 - acc: 0.542 - ETA: 2:29 - loss: 1.1657 - acc: 0.542 - ETA: 2:29 - loss: 1.1650 - acc: 0.542 - ETA: 2:28 - loss: 1.1639 - acc: 0.542 - ETA: 2:28 - loss: 1.1626 - acc: 0.543 - ETA: 2:28 - loss: 1.1615 - acc: 0.543 - ETA: 2:27 - loss: 1.1606 - acc: 0.543 - ETA: 2:27 - loss: 1.1597 - acc: 0.543 - ETA: 2:26 - loss: 1.1587 - acc: 0.544 - ETA: 2:26 - loss: 1.1579 - acc: 0.544 - ETA: 2:25 - loss: 1.1574 - acc: 0.544 - ETA: 2:25 - loss: 1.1560 - acc: 0.545 - ETA: 2:25 - loss: 1.1552 - acc: 0.545 - ETA: 2:24 - loss: 1.1542 - acc: 0.545 - ETA: 2:24 - loss: 1.1530 - acc: 0.546 - ETA: 2:23 - loss: 1.1518 - acc: 0.546 - ETA: 2:23 - loss: 1.1505 - acc: 0.547 - ETA: 2:22 - loss: 1.1499 - acc: 0.547 - ETA: 2:22 - loss: 1.1496 - acc: 0.547 - ETA: 2:22 - loss: 1.1489 - acc: 0.547 - ETA: 2:21 - loss: 1.1475 - acc: 0.547 - ETA: 2:21 - loss: 1.1467 - acc: 0.548 - ETA: 2:20 - loss: 1.1463 - acc: 0.548 - ETA: 2:20 - loss: 1.1455 - acc: 0.548 - ETA: 2:19 - loss: 1.1447 - acc: 0.548 - ETA: 2:19 - loss: 1.1436 - acc: 0.549 - ETA: 2:18 - loss: 1.1431 - acc: 0.549 - ETA: 2:18 - loss: 1.1420 - acc: 0.550 - ETA: 2:18 - loss: 1.1415 - acc: 0.550 - ETA: 2:17 - loss: 1.1408 - acc: 0.550 - ETA: 2:17 - loss: 1.1403 - acc: 0.550 - ETA: 2:16 - loss: 1.1395 - acc: 0.550 - ETA: 2:16 - loss: 1.1387 - acc: 0.551 - ETA: 2:15 - loss: 1.1380 - acc: 0.551 - ETA: 2:15 - loss: 1.1372 - acc: 0.551 - ETA: 2:15 - loss: 1.1365 - acc: 0.552 - ETA: 2:14 - loss: 1.1359 - acc: 0.552 - ETA: 2:14 - loss: 1.1352 - acc: 0.552 - ETA: 2:13 - loss: 1.1343 - acc: 0.552 - ETA: 2:13 - loss: 1.1338 - acc: 0.553 - ETA: 2:12 - loss: 1.1328 - acc: 0.553 - ETA: 2:12 - loss: 1.1317 - acc: 0.553 - ETA: 2:12 - loss: 1.1311 - acc: 0.554 - ETA: 2:11 - loss: 1.1300 - acc: 0.554 - ETA: 2:11 - loss: 1.1293 - acc: 0.554 - ETA: 2:10 - loss: 1.1281 - acc: 0.554 - ETA: 2:10 - loss: 1.1275 - acc: 0.555 - ETA: 2:09 - loss: 1.1263 - acc: 0.555 - ETA: 2:09 - loss: 1.1255 - acc: 0.555 - ETA: 2:08 - loss: 1.1243 - acc: 0.556 - ETA: 2:08 - loss: 1.1233 - acc: 0.556 - ETA: 2:08 - loss: 1.1225 - acc: 0.556 - ETA: 2:07 - loss: 1.1217 - acc: 0.556 - ETA: 2:07 - loss: 1.1206 - acc: 0.556 - ETA: 2:06 - loss: 1.1197 - acc: 0.557 - ETA: 2:06 - loss: 1.1192 - acc: 0.557 - ETA: 2:05 - loss: 1.1184 - acc: 0.557 - ETA: 2:05 - loss: 1.1179 - acc: 0.558 - ETA: 2:05 - loss: 1.1175 - acc: 0.558 - ETA: 2:04 - loss: 1.1172 - acc: 0.558 - ETA: 2:04 - loss: 1.1166 - acc: 0.5587"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105216/124848 [========================>.....] - ETA: 2:03 - loss: 1.1161 - acc: 0.559 - ETA: 2:03 - loss: 1.1152 - acc: 0.559 - ETA: 2:02 - loss: 1.1145 - acc: 0.559 - ETA: 2:02 - loss: 1.1145 - acc: 0.559 - ETA: 2:02 - loss: 1.1143 - acc: 0.559 - ETA: 2:01 - loss: 1.1137 - acc: 0.560 - ETA: 2:01 - loss: 1.1132 - acc: 0.560 - ETA: 2:00 - loss: 1.1126 - acc: 0.560 - ETA: 2:00 - loss: 1.1118 - acc: 0.560 - ETA: 1:59 - loss: 1.1113 - acc: 0.560 - ETA: 1:59 - loss: 1.1104 - acc: 0.561 - ETA: 1:59 - loss: 1.1092 - acc: 0.561 - ETA: 1:58 - loss: 1.1090 - acc: 0.561 - ETA: 1:58 - loss: 1.1085 - acc: 0.561 - ETA: 1:57 - loss: 1.1079 - acc: 0.562 - ETA: 1:57 - loss: 1.1069 - acc: 0.562 - ETA: 1:56 - loss: 1.1061 - acc: 0.562 - ETA: 1:56 - loss: 1.1056 - acc: 0.562 - ETA: 1:56 - loss: 1.1047 - acc: 0.563 - ETA: 1:55 - loss: 1.1040 - acc: 0.563 - ETA: 1:55 - loss: 1.1033 - acc: 0.563 - ETA: 1:54 - loss: 1.1023 - acc: 0.563 - ETA: 1:54 - loss: 1.1017 - acc: 0.563 - ETA: 1:53 - loss: 1.1013 - acc: 0.564 - ETA: 1:53 - loss: 1.1006 - acc: 0.564 - ETA: 1:53 - loss: 1.1001 - acc: 0.564 - ETA: 1:52 - loss: 1.0994 - acc: 0.564 - ETA: 1:52 - loss: 1.0988 - acc: 0.564 - ETA: 1:51 - loss: 1.0979 - acc: 0.564 - ETA: 1:51 - loss: 1.0975 - acc: 0.565 - ETA: 1:50 - loss: 1.0967 - acc: 0.565 - ETA: 1:50 - loss: 1.0959 - acc: 0.565 - ETA: 1:50 - loss: 1.0955 - acc: 0.565 - ETA: 1:49 - loss: 1.0952 - acc: 0.565 - ETA: 1:49 - loss: 1.0948 - acc: 0.566 - ETA: 1:48 - loss: 1.0939 - acc: 0.566 - ETA: 1:48 - loss: 1.0933 - acc: 0.566 - ETA: 1:47 - loss: 1.0924 - acc: 0.566 - ETA: 1:47 - loss: 1.0922 - acc: 0.566 - ETA: 1:46 - loss: 1.0917 - acc: 0.566 - ETA: 1:46 - loss: 1.0912 - acc: 0.567 - ETA: 1:46 - loss: 1.0908 - acc: 0.567 - ETA: 1:45 - loss: 1.0903 - acc: 0.567 - ETA: 1:45 - loss: 1.0894 - acc: 0.567 - ETA: 1:44 - loss: 1.0886 - acc: 0.567 - ETA: 1:44 - loss: 1.0879 - acc: 0.568 - ETA: 1:44 - loss: 1.0877 - acc: 0.568 - ETA: 1:43 - loss: 1.0869 - acc: 0.568 - ETA: 1:43 - loss: 1.0863 - acc: 0.568 - ETA: 1:43 - loss: 1.0859 - acc: 0.568 - ETA: 1:42 - loss: 1.0852 - acc: 0.568 - ETA: 1:42 - loss: 1.0847 - acc: 0.569 - ETA: 1:42 - loss: 1.0841 - acc: 0.569 - ETA: 1:41 - loss: 1.0836 - acc: 0.569 - ETA: 1:41 - loss: 1.0828 - acc: 0.569 - ETA: 1:41 - loss: 1.0825 - acc: 0.570 - ETA: 1:40 - loss: 1.0818 - acc: 0.570 - ETA: 1:40 - loss: 1.0810 - acc: 0.570 - ETA: 1:39 - loss: 1.0806 - acc: 0.570 - ETA: 1:39 - loss: 1.0797 - acc: 0.571 - ETA: 1:38 - loss: 1.0791 - acc: 0.571 - ETA: 1:38 - loss: 1.0785 - acc: 0.571 - ETA: 1:37 - loss: 1.0785 - acc: 0.571 - ETA: 1:37 - loss: 1.0779 - acc: 0.571 - ETA: 1:37 - loss: 1.0772 - acc: 0.572 - ETA: 1:36 - loss: 1.0769 - acc: 0.572 - ETA: 1:36 - loss: 1.0761 - acc: 0.572 - ETA: 1:35 - loss: 1.0758 - acc: 0.572 - ETA: 1:35 - loss: 1.0752 - acc: 0.572 - ETA: 1:34 - loss: 1.0749 - acc: 0.572 - ETA: 1:34 - loss: 1.0741 - acc: 0.572 - ETA: 1:33 - loss: 1.0740 - acc: 0.572 - ETA: 1:33 - loss: 1.0737 - acc: 0.573 - ETA: 1:33 - loss: 1.0732 - acc: 0.573 - ETA: 1:32 - loss: 1.0729 - acc: 0.573 - ETA: 1:32 - loss: 1.0722 - acc: 0.573 - ETA: 1:31 - loss: 1.0717 - acc: 0.573 - ETA: 1:31 - loss: 1.0711 - acc: 0.573 - ETA: 1:31 - loss: 1.0704 - acc: 0.573 - ETA: 1:30 - loss: 1.0699 - acc: 0.574 - ETA: 1:30 - loss: 1.0692 - acc: 0.574 - ETA: 1:30 - loss: 1.0684 - acc: 0.574 - ETA: 1:29 - loss: 1.0679 - acc: 0.574 - ETA: 1:29 - loss: 1.0674 - acc: 0.575 - ETA: 1:28 - loss: 1.0667 - acc: 0.575 - ETA: 1:28 - loss: 1.0658 - acc: 0.575 - ETA: 1:27 - loss: 1.0649 - acc: 0.576 - ETA: 1:27 - loss: 1.0646 - acc: 0.576 - ETA: 1:27 - loss: 1.0642 - acc: 0.576 - ETA: 1:26 - loss: 1.0638 - acc: 0.576 - ETA: 1:26 - loss: 1.0634 - acc: 0.576 - ETA: 1:26 - loss: 1.0632 - acc: 0.576 - ETA: 1:25 - loss: 1.0624 - acc: 0.577 - ETA: 1:25 - loss: 1.0618 - acc: 0.577 - ETA: 1:24 - loss: 1.0613 - acc: 0.577 - ETA: 1:24 - loss: 1.0604 - acc: 0.578 - ETA: 1:23 - loss: 1.0602 - acc: 0.578 - ETA: 1:23 - loss: 1.0596 - acc: 0.578 - ETA: 1:23 - loss: 1.0590 - acc: 0.578 - ETA: 1:22 - loss: 1.0585 - acc: 0.578 - ETA: 1:22 - loss: 1.0583 - acc: 0.578 - ETA: 1:21 - loss: 1.0577 - acc: 0.579 - ETA: 1:21 - loss: 1.0570 - acc: 0.579 - ETA: 1:20 - loss: 1.0566 - acc: 0.579 - ETA: 1:20 - loss: 1.0562 - acc: 0.579 - ETA: 1:19 - loss: 1.0553 - acc: 0.579 - ETA: 1:19 - loss: 1.0549 - acc: 0.579 - ETA: 1:18 - loss: 1.0542 - acc: 0.580 - ETA: 1:18 - loss: 1.0537 - acc: 0.580 - ETA: 1:18 - loss: 1.0532 - acc: 0.580 - ETA: 1:17 - loss: 1.0527 - acc: 0.580 - ETA: 1:17 - loss: 1.0523 - acc: 0.580 - ETA: 1:16 - loss: 1.0519 - acc: 0.580 - ETA: 1:16 - loss: 1.0515 - acc: 0.581 - ETA: 1:15 - loss: 1.0512 - acc: 0.581 - ETA: 1:15 - loss: 1.0509 - acc: 0.581 - ETA: 1:14 - loss: 1.0505 - acc: 0.581 - ETA: 1:14 - loss: 1.0502 - acc: 0.581 - ETA: 1:14 - loss: 1.0495 - acc: 0.581 - ETA: 1:13 - loss: 1.0490 - acc: 0.581 - ETA: 1:13 - loss: 1.0485 - acc: 0.582 - ETA: 1:12 - loss: 1.0479 - acc: 0.582 - ETA: 1:12 - loss: 1.0475 - acc: 0.582 - ETA: 1:11 - loss: 1.0471 - acc: 0.582 - ETA: 1:11 - loss: 1.0466 - acc: 0.582 - ETA: 1:10 - loss: 1.0460 - acc: 0.583 - ETA: 1:10 - loss: 1.0456 - acc: 0.583 - ETA: 1:09 - loss: 1.0453 - acc: 0.583 - ETA: 1:09 - loss: 1.0449 - acc: 0.583 - ETA: 1:09 - loss: 1.0444 - acc: 0.583 - ETA: 1:08 - loss: 1.0440 - acc: 0.583 - ETA: 1:08 - loss: 1.0436 - acc: 0.584 - ETA: 1:07 - loss: 1.0434 - acc: 0.584 - ETA: 1:07 - loss: 1.0430 - acc: 0.584 - ETA: 1:06 - loss: 1.0427 - acc: 0.584 - ETA: 1:06 - loss: 1.0423 - acc: 0.584 - ETA: 1:05 - loss: 1.0421 - acc: 0.584 - ETA: 1:05 - loss: 1.0417 - acc: 0.584 - ETA: 1:05 - loss: 1.0414 - acc: 0.584 - ETA: 1:04 - loss: 1.0413 - acc: 0.584 - ETA: 1:04 - loss: 1.0410 - acc: 0.584 - ETA: 1:03 - loss: 1.0405 - acc: 0.585 - ETA: 1:03 - loss: 1.0400 - acc: 0.585 - ETA: 1:02 - loss: 1.0398 - acc: 0.585 - ETA: 1:02 - loss: 1.0397 - acc: 0.585 - ETA: 1:01 - loss: 1.0393 - acc: 0.585 - ETA: 1:01 - loss: 1.0387 - acc: 0.585 - ETA: 1:00 - loss: 1.0386 - acc: 0.585 - ETA: 1:00 - loss: 1.0382 - acc: 0.585 - ETA: 1:00 - loss: 1.0376 - acc: 0.586 - ETA: 59s - loss: 1.0372 - acc: 0.586 - ETA: 59s - loss: 1.0367 - acc: 0.58 - ETA: 58s - loss: 1.0363 - acc: 0.58 - ETA: 58s - loss: 1.0361 - acc: 0.58 - ETA: 57s - loss: 1.0357 - acc: 0.58 - ETA: 57s - loss: 1.0356 - acc: 0.58 - ETA: 56s - loss: 1.0350 - acc: 0.58 - ETA: 56s - loss: 1.0345 - acc: 0.58 - ETA: 56s - loss: 1.0341 - acc: 0.58 - ETA: 55s - loss: 1.0339 - acc: 0.58 - ETA: 55s - loss: 1.0338 - acc: 0.58 - ETA: 54s - loss: 1.0335 - acc: 0.58 - ETA: 54s - loss: 1.0332 - acc: 0.58 - ETA: 53s - loss: 1.0330 - acc: 0.58 - ETA: 53s - loss: 1.0329 - acc: 0.58 - ETA: 52s - loss: 1.0327 - acc: 0.58 - ETA: 52s - loss: 1.0324 - acc: 0.58 - ETA: 51s - loss: 1.0320 - acc: 0.58 - ETA: 51s - loss: 1.0316 - acc: 0.58 - ETA: 51s - loss: 1.0313 - acc: 0.58 - ETA: 50s - loss: 1.0310 - acc: 0.58 - ETA: 50s - loss: 1.0310 - acc: 0.58 - ETA: 49s - loss: 1.0306 - acc: 0.58 - ETA: 49s - loss: 1.0300 - acc: 0.58 - ETA: 48s - loss: 1.0299 - acc: 0.58 - ETA: 48s - loss: 1.0294 - acc: 0.58 - ETA: 47s - loss: 1.0291 - acc: 0.58 - ETA: 47s - loss: 1.0289 - acc: 0.58 - ETA: 47s - loss: 1.0285 - acc: 0.58 - ETA: 46s - loss: 1.0281 - acc: 0.58 - ETA: 46s - loss: 1.0276 - acc: 0.58 - ETA: 45s - loss: 1.0274 - acc: 0.58 - ETA: 45s - loss: 1.0271 - acc: 0.58 - ETA: 44s - loss: 1.0266 - acc: 0.59 - ETA: 44s - loss: 1.0259 - acc: 0.59 - ETA: 43s - loss: 1.0256 - acc: 0.59 - ETA: 43s - loss: 1.0254 - acc: 0.59 - ETA: 42s - loss: 1.0250 - acc: 0.59 - ETA: 42s - loss: 1.0248 - acc: 0.59 - ETA: 42s - loss: 1.0245 - acc: 0.59 - ETA: 41s - loss: 1.0242 - acc: 0.59 - ETA: 41s - loss: 1.0238 - acc: 0.59 - ETA: 40s - loss: 1.0234 - acc: 0.59 - ETA: 40s - loss: 1.0235 - acc: 0.59 - ETA: 39s - loss: 1.0233 - acc: 0.59 - ETA: 39s - loss: 1.0229 - acc: 0.59 - ETA: 38s - loss: 1.0227 - acc: 0.59 - ETA: 38s - loss: 1.0226 - acc: 0.59 - ETA: 38s - loss: 1.0224 - acc: 0.59 - ETA: 37s - loss: 1.0222 - acc: 0.59 - ETA: 37s - loss: 1.0217 - acc: 0.59 - ETA: 36s - loss: 1.0213 - acc: 0.59 - ETA: 36s - loss: 1.0208 - acc: 0.59 - ETA: 35s - loss: 1.0207 - acc: 0.59 - ETA: 35s - loss: 1.0205 - acc: 0.59 - ETA: 34s - loss: 1.0200 - acc: 0.59 - ETA: 34s - loss: 1.0198 - acc: 0.5922"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124848/124848 [==============================] - ETA: 33s - loss: 1.0196 - acc: 0.59 - ETA: 33s - loss: 1.0193 - acc: 0.59 - ETA: 33s - loss: 1.0191 - acc: 0.59 - ETA: 32s - loss: 1.0190 - acc: 0.59 - ETA: 32s - loss: 1.0188 - acc: 0.59 - ETA: 31s - loss: 1.0185 - acc: 0.59 - ETA: 31s - loss: 1.0182 - acc: 0.59 - ETA: 30s - loss: 1.0178 - acc: 0.59 - ETA: 30s - loss: 1.0176 - acc: 0.59 - ETA: 29s - loss: 1.0173 - acc: 0.59 - ETA: 29s - loss: 1.0170 - acc: 0.59 - ETA: 29s - loss: 1.0167 - acc: 0.59 - ETA: 28s - loss: 1.0163 - acc: 0.59 - ETA: 28s - loss: 1.0160 - acc: 0.59 - ETA: 27s - loss: 1.0157 - acc: 0.59 - ETA: 27s - loss: 1.0153 - acc: 0.59 - ETA: 26s - loss: 1.0152 - acc: 0.59 - ETA: 26s - loss: 1.0149 - acc: 0.59 - ETA: 25s - loss: 1.0145 - acc: 0.59 - ETA: 25s - loss: 1.0142 - acc: 0.59 - ETA: 24s - loss: 1.0141 - acc: 0.59 - ETA: 24s - loss: 1.0135 - acc: 0.59 - ETA: 24s - loss: 1.0132 - acc: 0.59 - ETA: 23s - loss: 1.0129 - acc: 0.59 - ETA: 23s - loss: 1.0127 - acc: 0.59 - ETA: 22s - loss: 1.0122 - acc: 0.59 - ETA: 22s - loss: 1.0119 - acc: 0.59 - ETA: 21s - loss: 1.0118 - acc: 0.59 - ETA: 21s - loss: 1.0115 - acc: 0.59 - ETA: 20s - loss: 1.0113 - acc: 0.59 - ETA: 20s - loss: 1.0112 - acc: 0.59 - ETA: 20s - loss: 1.0108 - acc: 0.59 - ETA: 19s - loss: 1.0105 - acc: 0.59 - ETA: 19s - loss: 1.0103 - acc: 0.59 - ETA: 18s - loss: 1.0100 - acc: 0.59 - ETA: 18s - loss: 1.0097 - acc: 0.59 - ETA: 17s - loss: 1.0093 - acc: 0.59 - ETA: 17s - loss: 1.0089 - acc: 0.59 - ETA: 16s - loss: 1.0086 - acc: 0.59 - ETA: 16s - loss: 1.0085 - acc: 0.59 - ETA: 16s - loss: 1.0079 - acc: 0.59 - ETA: 15s - loss: 1.0077 - acc: 0.59 - ETA: 15s - loss: 1.0074 - acc: 0.59 - ETA: 14s - loss: 1.0071 - acc: 0.59 - ETA: 14s - loss: 1.0069 - acc: 0.59 - ETA: 13s - loss: 1.0065 - acc: 0.59 - ETA: 13s - loss: 1.0061 - acc: 0.59 - ETA: 12s - loss: 1.0059 - acc: 0.59 - ETA: 12s - loss: 1.0056 - acc: 0.59 - ETA: 11s - loss: 1.0054 - acc: 0.59 - ETA: 11s - loss: 1.0052 - acc: 0.59 - ETA: 11s - loss: 1.0049 - acc: 0.59 - ETA: 10s - loss: 1.0046 - acc: 0.59 - ETA: 10s - loss: 1.0044 - acc: 0.59 - ETA: 9s - loss: 1.0042 - acc: 0.5977 - ETA: 9s - loss: 1.0041 - acc: 0.597 - ETA: 8s - loss: 1.0037 - acc: 0.597 - ETA: 8s - loss: 1.0034 - acc: 0.597 - ETA: 7s - loss: 1.0032 - acc: 0.597 - ETA: 7s - loss: 1.0027 - acc: 0.598 - ETA: 7s - loss: 1.0023 - acc: 0.598 - ETA: 6s - loss: 1.0020 - acc: 0.598 - ETA: 6s - loss: 1.0017 - acc: 0.598 - ETA: 5s - loss: 1.0016 - acc: 0.598 - ETA: 5s - loss: 1.0015 - acc: 0.598 - ETA: 4s - loss: 1.0014 - acc: 0.598 - ETA: 4s - loss: 1.0012 - acc: 0.598 - ETA: 3s - loss: 1.0010 - acc: 0.598 - ETA: 3s - loss: 1.0007 - acc: 0.598 - ETA: 3s - loss: 1.0005 - acc: 0.598 - ETA: 2s - loss: 1.0002 - acc: 0.598 - ETA: 2s - loss: 1.0000 - acc: 0.598 - ETA: 1s - loss: 0.9997 - acc: 0.599 - ETA: 1s - loss: 0.9993 - acc: 0.599 - ETA: 0s - loss: 0.9990 - acc: 0.599 - ETA: 0s - loss: 0.9988 - acc: 0.599 - 232s 2ms/step - loss: 0.9987 - acc: 0.5994 - val_loss: 0.8525 - val_acc: 0.6438\n"
     ]
    }
   ],
   "source": [
    "#This is done for learning purpose only. One can play around with different hyper parameters combinations\n",
    "#and try increase the accuracy even more. For example, a different learning rate, an extra dense layer \n",
    "# before output layer, etc. Cross validation could be used to evaluate the model and grid search \n",
    "# further to find unique combination of parameters that give maximum accuracy. This model has a validation\n",
    "#accuracy of around 64.6%\n",
    "history=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=1, batch_size=256, verbose=1, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
